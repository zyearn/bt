%%==========================
%% chapter01.tex for SJTU Master Thesis
%% based on CASthesis
%% modified by wei.jianwen@gmail.com
%% version: 0.3a
%% Encoding: UTF-8
%% last update: Dec 5th, 2010
%%==================================================

%\bibliographystyle{sjtu2} %[此处用于每章都生产参考文献]
\chapter{系统实现}
\label{chap:impl}

\section{实现背景}
\label{sec:backgroud}

基于大数据时代的到来，本文提出的算法具有实际的应用价值，所以在本文的最后，将详细阐述我们是怎么实现模拟算法，并且给出一个能提供基础服务的架构原型，供用户能简单地使用。为了把重点放在算法上，我们默认用户上传的文件已经是一个经过分块文件块。

\section{系统架构}
\label{sec:arch}

在整体架构的设计中，我们采用普遍的C/S模型作为我们的服务模型。

\subsection{Client端}

我们的client定义为常见的浏览器，例如chrome，firefox，safari。访问服务的方法为登陆后端实现的网站，会显示需要上传的文件提示，按指示操作即可。

\subsection{Server端}

在实现server的时候，我们面临两个选择，一个是用C写底层的代码来实现一个server，这里首先需要实现实现一个server的最基本的功能，即能在端口上监听TCP套接字。之后就是HTTP协议的解析，全部需要自己手工来做，比较麻烦但优点是能对整个实现的掌握得非常好，包括内存资源，底层优化等等。另外一个选择是使用最近几年刚处的Nodejs来当Server，优点是有现成的框架，并且不用写底层的代码，都是封装起来的库，编写代码更加方便和快捷，缺点就是没有C那么直白，可能编程者无法对底层的实现了解得一清二楚，有时候在调试的时候可能至关重要。本文不拘泥于到底选择哪一种实现方式，因为这并不是我们关心的重点，我们关心的是模拟核心算法的实现。

在这里我们需要了解的是，我们的Server只接受两种类型的请求：

\begin{enumerate}
\item GET请求

这个请求用来向服务器读取index首页，首页的内容是提示用户上传一个文件。首页里有一个form，动作是post，用来向服务器提交数据。

\item POST请求

这个请求真正向服务器提交文件的内容，收到内容以后调用主要算法的逻辑模块，输出一个N维向量，然后和本地已有的所有文件的N维向量做比较，若是新文件，返回一个标示说明已存储；若是重复文件，则返回一个标示说明是重复文件。
\end{enumerate}

\subsection{生成模拟数据}

实现测试时使用的所有数据都是用linux下的dd命令实现，输入流是/dev/urandom来实现随机数据流。由于在仿真中我们省去了文件分块这一步，所有的文件是已经满足系统大小的文件块，所以直接进行到预处理的过程。在仿真中数据的简易性，我们设定系统的文件块大小为1KB。

\subsection{预处理}

预处理的算法是直接把输入文件的每一个字节看成是离散的点，FFT的输入一个复数数组，我们把每一个字节看作是一个复数的实部，虚部为0，代码如下所示。

\begin{lstlisting}[language={C}, caption={预处理}]
complex *pInput = new complex[length];
for(size_t i=0; i < length; i++) 
{
	pInput[i] = (double)input_file[i];
}
\end{lstlisting}

\subsection{FFT运算}

本仿真中FFT用了外部的库，所有者是LIBROW，这个库高效地实现了正向FFT和逆向FFT。使用起来也非常方便，如下代码，就是进行了一次正向的FFT变换。
\begin{lstlisting}[language={C}, caption={预处理}]
FFT::Forward(pSignal, length);
\end{lstlisting}

\subsection{向量的相似度比较}

我们使用了对两个向量每一维计算比值，在设定范围内，视为有效维，最后的相似度的定义就是有效维与所有维数的比值。代码在上一章中已经展示。

\section{与主流方法的比较}

我们将比较基于FFT的文件去重算法和传统的基于哈希方法的优缺点。我们先定义两个指标值，一个是查全率（recall），定义为正确去重的数量与存在的重复的比值；另一个是准确率（precision），定义为正确去重的数量与系统去掉的重复。查全率和准确率是一对矛盾，一味的追求查全率可能导致准备率的下降，反之亦然。

在查全率上，基于FFT的算法显然高于传统做法，因为我们允许了了系统参数$\epsilon$的存在，所以传统能检测到的，基于FFT的算法也可以检测到，但是那些相似度非常高的文件，传统方法无法检测出来，而FFT算法可以检测出，这就是为什么基于FFT的算法比传统算法好的根本原因。

在准确率上，基于FFT的算法低于传统做法，原因也是因为系统参数$\epsilon$的存在，使得一些不相同，但相似度正好也落在接受范围的可能性存在，但是仍在可接受的范围内。

\section{小结}
\label{sec:cons}

同传统的基于哈希值的去重算法比较起来，基于快速傅里叶变换的去重算法能够在文件发生修改的时候兼顾查全率和去重率，因此更适合使用不同场景要求的文件去重。在系统实现中，我们用C++写了主要的实现代码，并且实现了一个server来接受请求。
