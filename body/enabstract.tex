%%==================================================
%% abstract.tex for SJTU Master Thesis
%% based on CASthesis
%% modified by wei.jianwen@gmail.com
%% version: 0.3a
%% Encoding: UTF-8
%% last update: Dec 5th, 2010
%%==================================================

\begin{englishabstract}

    With the development of today's Internet and technology, the data capacity is growing at a fast rate. The research from the international data company(IDC) implies that by 2008 the amount of data was 0.8ZB, by 2009 the amount of data was 1.2ZB and by 2011 the amount of data reached up to 1.82ZB, which equals to the amount if everyone in the world generated more than 200GB data. We have entered a world of big data, with the rapid growing of data needed management. A research from IBM shows that among all the data in the human world, 90\% of them are generated in the last two years, and it is estimated that by 2020, the amount of data is 44 times larger than what we have today.

    Big Data has four significant features which are large, wide, low value-density, high timeliness. In this situation, users will generate vast amounts of data each day which causes the traditional disk-based file system has been unable to meet the needs of reality, such as that poor scalability and fault tolerance is poor, and it will be replaced by a distributed file system. Distributed File System refers to the file system resources not managed locally, but connected through a computer network with the local node. With the gradual growth of the distributed file system, many domestic Internet companies have adopted such a framework, and made a lot of very challenging problem. As used herein, improvement has made to the value of low-density which results in repeated uploaded repeated storage, flow and a great waste of space.

    In the background of such situation, there are many duplicated data in the Internet, causing duplicated uploading and duplicated storage which will waste plenty of Internet flow and storage. The topic of our project will focus on how to find out and deduplicate the data, and we also will improve the existing method and propose a Innovative method to solve this kind of question. Our main work and innovation are as follows.

\begin{enumerate}
    \item we compare the existing methods in the industry and survey the technique used by some well-known companies. We analyze these methods and give some shortcomings. A lot of companies use Hash method as the feature of the file, which means if the hash value is the same, the original file is also the same, which in most situations is correct, but not for all situations. For example, with the growing amount of data, the hash method used by some algorithms is vulnerable in some collision attacks. MD5 is cracked by professor Xiaoyun Wang in 2004. What's more, all these methods can not used to calculate the similarity of two files, making them not suitable for all situations. We innovatively propose a similarity judgement of file based on FFT.

    \item we propose a new data deduplication method based on FFT. It is different from the traditional methods because we provide a system parameter $\epsilon$ to adapt to nearly all situations. This parameter can vary from situation to situation. It depends on the system requirements for the degree of similarity. For example, there is such a situation that if any of the characters in the two files are different, then it is not considered as the same file. Another situation is that if the similarity of the two files within an acceptable range, then it is considered to be duplicate files. The main technique in this algorithm involves the preprocessing, the meaning of applying FFT to the file, the similarity judgement of N dimension array. 
        
        The preprocessing is a very important part in the algorithm. In this article, we directly use each byte of the file as a discrete points, each point in the range of 0-255, as an input in the real part of the complex number, which means, if the input is a 1MB file, the output is a 1 * 1024 * 1024 dimensional complex vector, wherein the real part $real(i)$ = the value corresponding to the i-th byte of the file, and the imaginary part $imag(i)$ = 0. 
        
        One difficulty of this paper is to understand that the file can be regarded as a bunch of discrete signal, which is a good way to analyze problem with innovation and change, and which is also the main reason why the Fourier transform can be applied in many fields (such as physics, structural dynamics, cryptography, etc.). If we look everything from abstract perspective, we can find they are all composed of a series of discrete dots. We put the pretreated vector to fast Fourier transform algorithm, and the N-dimensional vector output can be seen as another file block expression. At the output of the N-dimensional vector, an amplitude value for each frequency point are determined by all discrete signal from the original file. 
        
        In fact, there is some problem if the method is based on the law of cosines algorithm, then we have two options: First, we can improve the existing cosine algorithm to let the values of all dimensions in the order of the same magnitude, because different data dimensions relationship is not that big. But it is difficult to find a way to achieve this purpose; second, we can design a new algorithm to achieve the same purpose. This article will take the second approach that the design of a new approach to calculate the similarity between two N-dimensional vectors.

        In chapter four we will discuss the architecture and the algorithm itself in detail. This algorithm makes all data deduplication scenarios possible.

    \item The compare the result of our method to the existing method, we mainly focus on the recall. In the experiment, we have a higher recall no matter what the value of $\epsilon$ is , and the graph is drawn in chapter 5. Based on the arrival of the era of big data, the proposed algorithm has practical value, so in the end, we will elaborate in this article how to achieve simulation algorithm, and gives a framework to provide basic services to the prototype for the purpose that the user can easily use. In order to focus on the algorithm itself, we regard the user file uploaded is in the form of block. We implement our prototype based on Client/Server framework to provide the basic services to users. we use nodejs to implement the web server, C++ to implement the core calculating module and V8 engines to enable the communication between javascript and C++.
\end{enumerate}

  \englishkeywords{\large data deduplication, big data, preprocess, FFT, evaluation, prototype implementation}
\end{englishabstract}
