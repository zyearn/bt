%%==================================================
%% conclusion.tex for SJTU Master Thesis
%% based on CASthesis
%% modified by wei.jianwen@gmail.com
%% version: 0.3a
%% Encoding: UTF-8
%% last update: Dec 5th, 2010
%%==================================================

\chapter*{全文总结\markboth{全文总结}{}}
\addcontentsline{toc}{chapter}{全文总结}

在绪论中，我们先阐述了课题的研究背景和研究内容，以及在当今环境下课题的重要性。之后详细调查了国内外的研究现状，并比较了现有做法的一些优缺点，之后阐述了全文的结构。

之后我们调研了重复数据检测的关键技术，包括全文件检测技术、简单分块技术、内容快技术、滑动块技术。每一个技术都是前一个技术的改进和提高，我们分析了这些技术中的核心要点，即基于哈希的算法实现，提出了这个算法的局限性，并根据此提出了一种基于相似度的文件去重算法。

基于FFT的相似文件去重涉及到了很多的核心技术，我们又详细说明了这些技术，包括从抽象的角度来理解FFT的作用，以及把文件看成离散的点这种创新性的思维方式；N维向量的相似度比较，我们提出了业界普遍使用了余弦定理，之后通过实验得出，余弦定理并不适合我们这个场景，然后又提出了一种新的向量相似度比较方式；以及基于内存计算的框架，我们比较了Hadoop和Spark的优缺点。

在介绍完核心技术以后，我们开始介绍基于FFT的文件去重算法，我们先介绍了分块的方法，本文采取的做法是简单分块的做法，即将文件按照固定长度分成一个个文件块的方法；接着是分块预处理，我们比较了几种现有的做法，并提出了一种简单高效的做法；我们讨论了对文本进行FFT变换的意义，这是理解本文的一个关键所在；之后我们又提出了基于系统参数$\epsilon$的N维向量的比较算法，最后我们将算法设计在了spark平台上已达到并行处理的目的。

文章的最后，我们实现了提出的算法，并且采用了Client/Server模型做了系统服务的最基础的原型。
